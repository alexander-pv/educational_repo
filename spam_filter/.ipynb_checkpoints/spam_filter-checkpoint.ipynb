{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Классификация текстов: спам-фильтр для SMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['readme', 'SMSSpamCollection.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('smsspamcollection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('smsspamcollection', 'readme')) as f:\n",
    "    readme_data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SMS Spam Collection v.1\\n',\n",
       " '-------------------------\\n',\n",
       " '\\n',\n",
       " '1. DESCRIPTION\\n',\n",
       " '--------------\\n',\n",
       " '\\n',\n",
       " 'The SMS Spam Collection v.1 (hereafter the corpus) is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged acording being ham (legitimate) or spam. \\n',\n",
       " '\\n',\n",
       " '1.1. Compilation\\n',\n",
       " '----------------\\n',\n",
       " '\\n',\n",
       " 'This corpus has been collected from free or free for research sources at the Web:\\n',\n",
       " '\\n',\n",
       " '- A collection of between 425 SMS spam messages extracted manually from the Grumbletext Web site. This is a UK forum in which cell phone users make public claims about SMS spam messages, most of them without reporting the very spam message received. The identification of the text of spam messages in the claims is a very hard and time-consuming task, and it involved carefully scanning hundreds of web pages. The Grumbletext Web site is: http://www.grumbletext.co.uk/\\n',\n",
       " \"- A list of 450 SMS ham messages collected from Caroline Tag's PhD Theses available at http://etheses.bham.ac.uk/253/1/Tagg09PhD.pdf\\n\",\n",
       " '- A subset of 3,375 SMS ham messages of the NUS SMS Corpus (NSC), which is a corpus of about 10,000 legitimate messages collected for research at the Department of Computer Science at the National University of Singapore. The messages largely originate from Singaporeans and mostly from students attending the University. These messages were collected from volunteers who were made aware that their contributions were going to be made publicly available. The NUS SMS Corpus is avalaible at: http://www.comp.nus.edu.sg/~rpnlpir/downloads/corpora/smsCorpus/\\n',\n",
       " '- The amount of 1,002 SMS ham messages and 322 spam messages extracted from the SMS Spam Corpus v.0.1 Big created by Josй Marнa Gуmez Hidalgo and public available at: http://www.esp.uem.es/jmgomez/smsspamcorpus/\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '1.2. Statistics\\n',\n",
       " '---------------\\n',\n",
       " '\\n',\n",
       " 'There is one collection:\\n',\n",
       " '\\n',\n",
       " '- The SMS Spam Collection v.1 (text file: smsspamcollection) has a total of 4,827 SMS legitimate messages (86.6%) and a total of 747 (13.4%) spam messages.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '1.3. Format\\n',\n",
       " '-----------\\n',\n",
       " '\\n',\n",
       " 'The files contain one message per line. Each line is composed by two columns: one with label (ham or spam) and other with the raw text. Here are some examples:\\n',\n",
       " '\\n',\n",
       " 'ham   What you doing?how are you?\\n',\n",
       " 'ham   Ok lar... Joking wif u oni...\\n',\n",
       " 'ham   dun say so early hor... U c already then say...\\n',\n",
       " 'ham   MY NO. IN LUTON 0125698789 RING ME IF UR AROUND! H*\\n',\n",
       " 'ham   Siva is in hostel aha:-.\\n',\n",
       " 'ham   Cos i was out shopping wif darren jus now n i called him 2 ask wat present he wan lor. Then he started guessing who i was wif n he finally guessed darren lor.\\n',\n",
       " 'spam   FreeMsg: Txt: CALL to No: 86888 & claim your reward of 3 hours talk time to use from your phone now! ubscribe6GBP/ mnth inc 3hrs 16 stop?txtStop\\n',\n",
       " 'spam   Sunshine Quiz! Win a super Sony DVD recorder if you canname the capital of Australia? Text MQUIZ to 82277. B\\n',\n",
       " 'spam   URGENT! Your Mobile No 07808726822 was awarded a L2,000 Bonus Caller Prize on 02/09/03! This is our 2nd attempt to contact YOU! Call 0871-872-9758 BOX95QU\\n',\n",
       " '\\n',\n",
       " 'Note: messages are not chronologically sorted.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '2. USAGE\\n',\n",
       " '--------\\n',\n",
       " '\\n',\n",
       " 'We offer a comprehensive study of this corpus in the following paper that is under review. This work presents a number of statistics, studies and baseline results for several machine learning methods.\\n',\n",
       " '\\n',\n",
       " \"[1] Almeida, T.A., Gуmez Hidalgo, J.M., Yamakami, A. Contributions to the study of SMS Spam Filtering: New Collection and Results. Proceedings of the 2011 ACM Symposium on Document Engineering (ACM DOCENG'11), Mountain View, CA, USA, 2011. (Under review)\\n\",\n",
       " '\\n',\n",
       " '\\n',\n",
       " '3. ABOUT\\n',\n",
       " '--------\\n',\n",
       " '\\n',\n",
       " 'The corpus has been collected by Tiago Agostinho de Almeida (http://www.dt.fee.unicamp.br/~tiago) and Josй Marнa Gуmez Hidalgo (http://www.esp.uem.es/jmgomez).\\n',\n",
       " '\\n',\n",
       " 'We would like to thank Dr. Min-Yen Kan (http://www.comp.nus.edu.sg/~kanmy/) and his team for making the NUS SMS Corpus available. See: http://www.comp.nus.edu.sg/~rpnlpir/downloads/corpora/smsCorpus/. He is currently collecting a bigger SMS corpus at: http://wing.comp.nus.edu.sg:8080/SMSCorpus/\\n',\n",
       " '\\n',\n",
       " '4. LICENSE/DISCLAIMER\\n',\n",
       " '---------------------\\n',\n",
       " '\\n',\n",
       " 'We would appreciate if:\\n',\n",
       " '\\n',\n",
       " '- In case you find this corpus useful, please make a reference to previous paper and the web page: http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/ in your papers, research, etc.\\n',\n",
       " '- Send us a message to tiago@dt.fee.unicamp.br in case you make use of the corpus.\\n',\n",
       " '\\n',\n",
       " 'The SMS Spam Collection v.1 is provided for free and with no limitations excepting:\\n',\n",
       " '\\n',\n",
       " '1. Tiago Agostinho de Almeida and Josй Marнa Gуmez Hidalgo hold the copyrigth (c) for the SMS Spam Collection v.1.\\n',\n",
       " '\\n',\n",
       " \"2. No Warranty/Use At Your Risk. THE CORPUS IS MADE AT NO CHARGE. ACCORDINGLY, THE CORPUS IS PROVIDED `AS IS,' WITHOUT WARRANTY OF ANY KIND, INCLUDING WITHOUT LIMITATION THE WARRANTIES THAT THEY ARE MERCHANTABLE, FIT FOR A PARTICULAR PURPOSE OR NON-INFRINGING. YOU ARE SOLELY RESPONSIBLE FOR YOUR USE, DISTRIBUTION, MODIFICATION, REPRODUCTION AND PUBLICATION OF THE CORPUS AND ANY DERIVATIVE WORKS THEREOF BY YOU AND ANY OF YOUR SUBLICENSEES (COLLECTIVELY, `YOUR CORPUS USE'). THE ENTIRE RISK AS TO YOUR CORPUS USE IS BORNE BY YOU. YOU AGREE TO INDEMNIFY AND HOLD THE COPYRIGHT HOLDERS, AND THEIR AFFILIATES HARMLESS FROM ANY CLAIMS ARISING FROM OR RELATING TO YOUR CORPUS USE.\\n\",\n",
       " '\\n',\n",
       " '3. Limitation of Liability. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR THEIR AFFILIATES, OR THE CORPUS CONTRIBUTING EDITORS, BE LIABLE FOR ANY INDIRECT, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES, INCLUDING, WITHOUT LIMITATION, DAMAGES FOR LOSS OF GOODWILL OR ANY AND ALL OTHER COMMERCIAL DAMAGES OR LOSSES, EVEN IF ADVISED OF THE POSSIBILITY THEREOF, AND REGARDLESS OF WHETHER ANY CLAIM IS BASED UPON ANY CONTRACT, TORT OR OTHER LEGAL OR EQUITABLE THEORY, RELATING OR ARISING FROM THE CORPUS, YOUR CORPUS USE OR THIS LICENSE AGREEMENT.\\n']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readme_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем весь датасет и посмотрим на его структуру"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(file=os.path.join('smsspamcollection', 'SMSSpamCollection.txt'), \n",
    "          encoding='utf-8'\n",
    "         ) as f:\n",
    "    data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ham\\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\\n',\n",
       " 'ham\\tOk lar... Joking wif u oni...\\n',\n",
       " \"spam\\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\\n\",\n",
       " 'ham\\tU dun say so early hor... U c already then say...\\n',\n",
       " \"ham\\tNah I don't think he goes to usf, he lives around here though\\n\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, разделители в данных- табуляция. В качестве меток классов используются слова `ham`, `spam` в начале каждого sms-сообщения. испоотльзуем pandas для быстрой обработки датасета и приведения в удобный формат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset = pd.read_csv(os.path.join('smsspamcollection', 'SMSSpamCollection.txt'), \n",
    "            sep='\\t', \n",
    "            names = ['spam', 'sms'])\n",
    "\n",
    "df_dataset['spam'] = df_dataset['spam'].apply(lambda x: {'spam': 1, 'ham': 0}[x])\n",
    "df_dataset['sms'] = df_dataset['sms'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   spam    5572 non-null   int64 \n",
      " 1   sms     5572 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 87.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spam</th>\n",
       "      <th>sms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>go until jurong point, crazy.. available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>ok lar... joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>u dun say so early hor... u c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>nah i don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   spam                                                sms\n",
       "0     0  go until jurong point, crazy.. available only ...\n",
       "1     0                      ok lar... joking wif u oni...\n",
       "2     1  free entry in 2 a wkly comp to win fa cup fina...\n",
       "3     0  u dun say so early hor... u c already then say...\n",
       "4     0  nah i don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вспомогательная функция для автоматической генерации ответов\n",
    "def write_answer(txt, name):\n",
    "    if not isinstance(txt, str):\n",
    "        txt = str(txt)\n",
    "    with open(f'{name}.txt', 'w') as f:\n",
    "        f.write(txt)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 1:__\n",
    "\n",
    "    Оцените качество классификации текстов с помощью LogisticRegression() с параметрами по умолчанию, используя sklearn.cross_validation.cross_val_score и посчитав среднее арифметическое качества на отдельных fold'ах. Установите random_state=2. Параметр cv задайте равным 10. В качестве метрики качества используйте f1-меру."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pipeline(model, vectorizer):\n",
    "    return Pipeline(\n",
    "            [(\"vectorizer\", vectorizer),\n",
    "             (\"classifier\", model)]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(LogisticRegression(random_state=2), CountVectorizer())\n",
    "result = cross_validate(model, \n",
    "                        X=df_dataset['sms'], y=df_dataset['spam'], \n",
    "                        cv=10, scoring='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_answer(round(result['test_score'].mean(), 1), 'q1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Среднее арифметическое F1-score на 10 фолдах простой модели составляет 0.93"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 2:__\n",
    "\n",
    "    А теперь обучите классификатор на всей выборке и спрогнозируйте с его помощью класс для следующих сообщений:\n",
    "    \"FreeMsg: Txt: CALL to No: 86888 & claim your reward of 3 hours talk time to use from your phone now! Subscribe6GB\"\n",
    "    \"FreeMsg: Txt: claim your reward of 3 hours talk time\"\n",
    "    \"Have you visited the last lecture on physics?\"\n",
    "    \"Have you visited the last lecture on physics? Just buy this book and you will have all materials! Only 99$\"\n",
    "    \"Only 99$\"\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = [\"FreeMsg: Txt: CALL to No: 86888 & claim your reward of 3 hours talk time to use from your phone now! Subscribe6GB\",\n",
    "            \"FreeMsg: Txt: claim your reward of 3 hours talk time\",\n",
    "            \"Have you visited the last lecture on physics?\",\n",
    "            \"Have you visited the last lecture on physics? Just buy this book and you will have all materials! Only 99$\",\n",
    "            \"Only 99$\"\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(LogisticRegression(random_state=2), CountVectorizer())\n",
    "model.fit(X=df_dataset['sms'], y=df_dataset['spam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_answer(' '.join([str(x) for x in model.predict(test_list)]), 'q2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 3:__\n",
    "\n",
    "    Задайте в CountVectorizer параметр ngram_range=(2,2), затем ngram_range=(3,3), затем ngram_range=(1,3). Во всех трех случаях измерьте получившееся в кросс-валидации значение f1-меры.\n",
    "    \n",
    "    В данном эксперименте мы пробовали добавлять в признаки n-граммы для разных диапазонов n - только биграммы, только триграммы, и, наконец, все вместе - униграммы, биграммы и триграммы. \n",
    "    \n",
    "    Обратите внимание, что статистики по биграммам и триграммам намного меньше, поэтому классификатор только на них работает хуже. В то же время это не ухудшает результат сколько-нибудь существенно, если добавлять их вместе с униграммами, т.к. за счет регуляризации линейный классификатор не склонен сильно переобучаться на этих признаках."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_scores = []\n",
    "for ngram_range in [(2,2), (3,3), (1,3)]:\n",
    "    model = make_pipeline(LogisticRegression(random_state=2),\n",
    "                          CountVectorizer(ngram_range=ngram_range))\n",
    "    result = cross_validate(model, \n",
    "                            X=df_dataset['sms'], y=df_dataset['spam'], \n",
    "                            cv=10, scoring='f1')\n",
    "    print(f\"\"\"ngram_range: {ngram_range}, f1-score: {result['test_score'].mean()}\"\"\")\n",
    "    f_scores.append(round(result['test_score'].mean(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_answer(' '.join([str(x) for x in f_scores]), 'q3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 4:__\n",
    "\n",
    "    Повторите аналогичный эксперимент, используя вместо логистической регрессии /()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_scores = []\n",
    "for ngram_range in [(2,2), (3,3), (1,3)]:\n",
    "    model = make_pipeline(MultinomialNB(),\n",
    "                          CountVectorizer(ngram_range=ngram_range))\n",
    "    result = cross_validate(model, \n",
    "                            X=df_dataset['sms'], y=df_dataset['spam'], \n",
    "                            cv=10, scoring='f1')\n",
    "    print(f\"\"\"ngram_range: {ngram_range}, f1-score: {result['test_score'].mean()}\"\"\")\n",
    "    f_scores.append(round(result['test_score'].mean(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Наблюдается аномальное поведение MultinomialNB в Pipeline\n",
    "f_scores = []\n",
    "for ngram_range in [(2,2), (3,3), (1,3)]:\n",
    "    model = MultinomialNB()\n",
    "    vectorizer = CountVectorizer(ngram_range=ngram_range)\n",
    "    x_transformed = vectorizer.fit_transform(df_dataset['sms'])\n",
    "    result = cross_validate(model, \n",
    "                            X=x_transformed, y=df_dataset['spam'], \n",
    "                            cv=10, scoring='f1')\n",
    "    print(f\"\"\"ngram_range: {ngram_range}, f1-score: {result['test_score'].mean()}\"\"\")\n",
    "    f_scores.append(round(result['test_score'].mean(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_answer(' '.join([str(x) for x in f_scores]), 'q4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 5:__\n",
    "\n",
    "\n",
    "    Попробуйте использовать в логистической регрессии в качестве признаков Tf*idf из TfidfVectorizer на униграммах. Повысилось или понизилось качество на кросс-валидации по сравнению с CountVectorizer на униграммах?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(LogisticRegression(random_state=2), TfidfVectorizer())\n",
    "result = cross_validate(model, \n",
    "                        X=df_dataset['sms'], y=df_dataset['spam'], \n",
    "                        cv=10, scoring='f1')\n",
    "print(result['test_score'])\n",
    "print(result['test_score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_answer(-1, 'q5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
